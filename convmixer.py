# -*- coding: utf-8 -*-
"""ConvMixer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WoWqQSgy-O0TAuO_nKSTFMIOOlnWlGg4
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, StratifiedKFold, train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import cv2
import math
from sklearn.preprocessing import LabelBinarizer
from sklearn import metrics
from tensorflow import keras
from keras import layers
from google.colab import files
import shutil

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

files.upload()  # This will prompt you to upload kaggle.json

os.makedirs('/root/.kaggle', exist_ok=True)

shutil.move('kaggle.json', '/root/.kaggle/kaggle.json')

os.chmod('/root/.kaggle/kaggle.json', 0o600)

!kaggle datasets download -d dhruvverma8291/cvdv-adni-1-full

from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir, confusion_matrix

unzip_data("/content/minor-adni-1-full.zip")

ROOT = '/content/final_training_dataset'
num_class = 2
BATCH_SIZE = 64

hyperparameters = {
    'learning_rate':0.001,
    'weight_decay':0.0001,
    'batch_size':32,
    'num_epochs':35,
    'image_size':(256,256),
    'patch_size':16,
    'num_patches':[256 // 4, 256 // 8, 256 // 16],
    'projection_dim': 64,
    'num_heads':[4,8,16],
    'transformer_units' : [
        128,
        64
    ],
    'transformer_layers': 8,
    'mlp_head_units':[
        2048,
        1024
    ],
    'meta_step_size':0.25,
    'inner_batch_size':25,
    'eval_batch_size':25
    'meta_iters':100,
    'eval_iters':5,
    'inner_iters':4,
    'eval_interval':1,
    'train_shots':20,
    'shots':5,
    'meta_classes':2
}

class Dataset():

    def __init__(self, root, batch_size=32, img_size=224, shuffle=False, KFold=False, OneHot = False):
        self.ROOT = root
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.KFold = KFold
        self.imgx = []
        self.labels = []
        self.img_size = img_size
        self.OneHot = OneHot

        for i in os.listdir(self.ROOT):
            path = os.path.join(self.ROOT,i)
            for j in os.listdir(path):
                self.imgx.append(os.path.join(path,j))
                self.labels.append(i)

        self.imgx = np.array(self.imgx)
        self.le = LabelEncoder()
        self.encoded_labels = self.le.fit_transform(self.labels)
        if self.OneHot == True:
            self.encoded_labels = tf.one_hot(self.encoded_labels,3)
        self.encoded_labels = np.array(self.encoded_labels)
        if self.OneHot == True:
            self.encoded_labels = LabelBinarizer.fit_transform(self.encoded_labels)
            print(self.encoded_labels[:5])
        if self.shuffle == True:
            indices = np.random.RandomState(42).permutation(len(self.imgx))
            self.imgx = self.imgx[indices]
            self.encoded_labels = self.encoded_labels[indices]

    def __read_and_preprocess_image__(self,image):

        img = tf.io.read_file(image)
        img = tf.io.decode_jpeg(img,channels=3)
        img = tf.cast(img,tf.float32) / 255.0
        img = tf.expand_dims(img,axis=0)
        img = tf.keras.layers.Cropping2D(cropping=((35,29),(20,20)))(img)
        img = tf.squeeze(img)
        img = tf.image.resize(img,[self.img_size,self.img_size])
        img = tf.image.per_image_standardization(img)
        return img

    def get_data(self):

        total = self.imgx.shape[0]
        num_loops = math.floor(self.imgx.shape[0] / self.batch_size)
        self.imgx = self.imgx[:-(total-self.batch_size*num_loops)]
        self.encoded_labels = self.encoded_labels[:-(total-self.batch_size*num_loops)]
        self.imgx = self.imgx.reshape(num_loops,self.batch_size)
        self.encoded_labels = self.encoded_labels.reshape(num_loops,self.batch_size,3)

        for batch_i,batch_l in zip(self.imgx,self.encoded_labels):
            for image,label in zip(batch_i,batch_l):
                image = self.__read_and_preprocess_image__(image)
                yield((image,label))

class DR():

    def __init__(self,one_hot=False):
        self.ROOT = ROOT
        self.folds = {}
        self.classes = ['MCI','CN']
        self.imgx = []
        self.label = []
        self.data_dict = {0:[],1:[]}
        self.encoded_classes = []
        self.mini_dataset = []
        self.counter = 0
        self.one_hot = one_hot
        self.le = LabelEncoder()

        for i in self.classes:
            path = os.path.join(self.ROOT,i)
            for j in os.listdir(path):
                self.imgx.append(os.path.join(path,j))
                self.label.append(i)
                self.data_dict[self.counter].append(os.path.join(path,j))
            self.counter+=1
        self.imgx = np.array(self.imgx)
        self.label = np.array(self.label)
        self.encoded_labels = self.le.fit_transform(self.label)
        if self.one_hot == True:
            self.encoded_labels = LabelBinarizer().fit_transform(self.encoded_labels)
            print(self.encoded_labels[:5])
        self.dataset = self.get_dataset(KFold=True)

    def __give_K_folds__(self,k):

        folds = {new_list:{'train':{'imgx':[],'label':[]},'test':{'imgx':[],'label':[]}} for new_list in range(k)}
        self.folds = folds
        kf = KFold(n_splits=k,shuffle=True,random_state=42)
        for i, (train_index, test_index) in enumerate(kf.split(self.imgx,self.encoded_labels)):
            self.folds[i]['train']['imgx'].append(self.imgx[train_index])
            self.folds[i]['train']['label'].append(self.encoded_labels[train_index])
            self.folds[i]['test']['imgx'].append(self.imgx[test_index])
            self.folds[i]['test']['label'].append(self.encoded_labels[test_index])
        return self.folds

    def __load_and_normalize_image__(self,image,label):

        img = tf.io.read_file(image)
        img = tf.io.decode_jpeg(img,channels=3)
        img = tf.image.resize(img,(256,256))
        img = tf.expand_dims(img,axis=0)
        img = tf.keras.layers.Cropping2D(cropping=((35,29),(20,20)))(img)
        img = tf.squeeze(img)
        img = tf.image.resize(img,(256,256))
        img = tf.cast(img,tf.float32)/255.
        return img, label

    def __load_and_normalize_image2__(self,image):

        img = tf.io.read_file(image)
        img = tf.io.decode_jpeg(img,channels=3)
        img = tf.image.resize(img,(224,224))
        img = tf.cast(img,tf.float32) / 255.0
        return img

    def get_dataset(self,val_split=0.2,Shuffle=False,KFold=False,K=5,BATCH_SIZE=32):

        if KFold == False:
            train_ds, validation_ds = tf.keras.utils.image_dataset_from_directory(self.ROOT,batch_size=BATCH_SIZE,image_size=(256,256),shuffle=Shuffle, seed=42, validation_split=val_split,subset='both')
            return {'train':train_ds, 'validation':validation_ds}
        else:
            dataset = {new_list:{'train':[],'validation':[]} for new_list in range(K)}
            folds = self.__give_K_folds__(K)
            for fold in folds.keys():
                train_dataset = tf.data.Dataset.from_tensor_slices((folds[fold]['train']['imgx'][0],folds[fold]['train']['label'][0]))
                train_dataset = train_dataset.map(self.__load_and_normalize_image__,tf.data.AUTOTUNE)
                test_dataset = tf.data.Dataset.from_tensor_slices((folds[fold]['test']['imgx'][0],folds[fold]['test']['label'][0]))
                test_dataset = test_dataset.map(self.__load_and_normalize_image__,tf.data.AUTOTUNE)
                dataset[fold]['train']=train_dataset
                dataset[fold]['validation'] = test_dataset
            self.dataset =  dataset
            return dataset

    def get_mini_dataset(self,fold,batch_size=32,repetitions=3, shots=5, num_classes=2, split=False):

        sub_indices = random.sample(range(len(self.encoded_classes)),k=num_classes)
        print(sub_indices)
        label_subset = np.array(self.encoded_classes)[sub_indices]
        meta_labels = np.zeros(num_classes*shots)
        meta_test_labels = np.zeros(num_classes)
        meta_img_paths = []
        meta_test_img_paths = []

        for class_idx, class_obj in enumerate(label_subset):
            meta_labels[class_idx*shots:(class_idx+1)*shots]=class_idx
            img_paths = random.sample(range(len(self.data_dict[class_obj])),k=shots+1)
            for i in img_paths:
                meta_img_paths.append(self.data_dict[class_obj][i])
            meta_test_img_paths.append(meta_img_paths[-1])
            meta_img_paths = meta_img_paths[:-1]
        meta_test_img_paths = np.array(meta_test_img_paths)
        meta_img_paths = np.array(meta_img_paths)

        test_images = []

        for i,j in zip(meta_test_img_paths,meta_test_labels):
            test_images.append(self.__load_and_normalize_image2__(i))

        train_dataset = tf.data.Dataset.from_tensor_slices((meta_img_paths,meta_labels))
        train_dataset = train_dataset.map(self.__load_and_normalize_image__,tf.data.AUTOTUNE)
        train = train_dataset.shuffle(100).batch(batch_size).repeat(repetitions)

        return train, test_images, meta_test_labels

dataset = DR().get_dataset(KFold=True)

def show_sample_images(num_samples=5,train=True,axis='off'):
    for counter, (i,j) in zip(range(num_samples),dataset[0]['train'].shuffle(6000)):
        plt.imshow(i)
        plt.axis(axis)
        plt.show()
        print(j)
show_sample_images()

#AD_samples = len(os.listdir(os.path.join(ROOT,'AD')))
MCI_samples = len(os.listdir(os.path.join(ROOT,'MCI')))
CN_samples = len(os.listdir(os.path.join(ROOT,'CN')))

total =  MCI_samples + CN_samples

print(MCI_samples, CN_samples, total)

MCI_weights = (1/MCI_samples)*(total/2.0)
# MCI_weights = (1/MCI_samples)*(total/2.0)
CN_weights = (1/CN_samples)*(total/2.0)

print(f'class weights for MCI: {MCI_weights}, CN: {CN_weights}')

class_weights = {0: MCI_weights, 1: CN_weights}

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
filepath = '/content/checkpoint.weights.h5',
monitor ='val_accuracy',
mode='max',
save_best_only = True,
save_weights_only = True
)

def activation_block(x):

    x = layers.Activation("gelu")(x)
    return layers.BatchNormalization()(x)

def conv_stem(x, filters: int, patch_size: int):

    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)
    return activation_block(x)

def conv_mixer_block(x, filters: int, kernel_size: int):

    # Depthwise convolution.
    x0 = x
    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding="same")(x)
    x = layers.Add()([activation_block(x), x0])  # Residual.

    # Pointwise convolution.

    x = layers.Conv2D(filters, kernel_size=1)(x)
    x = activation_block(x)

    return x

def get_conv_mixer_256_8(

    image_size=256, filters=256, depth=8, kernel_size=5, patch_size=4, num_classes=2

):

    """ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.

    The hyperparameter values are taken from the paper.

    """

    inputs = keras.Input((image_size, image_size, 3))

    # x = layers.Rescaling(scale=1.0 / 255)(inputs)

    x = conv_stem(inputs, filters, patch_size)

    # ConvMixer blocks.

    for _ in range(depth):
        x = conv_mixer_block(x, filters, kernel_size)

    # Classification block.

    x = layers.GlobalAvgPool2D()(x)
    x = layers.Dropout(.3)(x)

    outputs = layers.Dense(num_classes, activation="softmax")(x)

    return keras.Model(inputs, outputs)

def run_experiment_with_class_weights(model, train_data, test_data):
    optimizer = tf.keras.optimizers.AdamW(
        learning_rate=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay']
    )

    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # Use the class, not the function
        metrics=["accuracy"]

    )

    history = model.fit(
        x=train_data,
        batch_size=hyperparameters['batch_size'],
        epochs=hyperparameters['num_epochs'],
        validation_data=test_data,
        class_weight = class_weights,
        callbacks=model_checkpoint_callback
    )

    return history

import gc
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import metrics

history = []

for fold in range(5):
    # Shuffle and batch datasets
    train_ds = dataset[fold]['train'].shuffle(6000).batch(hyperparameters['batch_size'])
    test_ds = dataset[fold]['validation'].shuffle(6000).batch(hyperparameters['batch_size'])

    print(f'<---------------- FOLD {fold} ------------------------------->')

    # Create a new ConvMixer model for each fold
    conv_mixer_model = get_conv_mixer_256_8(256)

    # Load weights from the previous fold
    if fold > 0:
        weight_path = f'/content/checkpoint_fold{fold-1}.weights.h5'
        try:
            conv_mixer_model.load_weights(weight_path)
            print(f"Successfully loaded weights for fold {fold} from {weight_path}")
        except Exception as e:
            print(f"Error loading weights: {e}")

    # Train the model
    h = run_experiment_with_class_weights(conv_mixer_model, train_ds, test_ds)

    # Save weights for this fold
    weight_save_path = f'/content/checkpoint_fold{fold}.weights.h5'
    conv_mixer_model.save_weights(weight_save_path)

    # Save training history as CSV
    df = pd.DataFrame.from_dict(h.history)
    history_save_path = f'/content/curves_fold{fold}.csv'
    df.to_csv(history_save_path, index=False)

    # Plot and save training progress
    plt.plot(h.history['accuracy'])
    plt.plot(h.history['val_accuracy'])
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'Training Progress for Fold {fold}')
    plt.legend(['Training', 'Validation'])
    training_progress_save_path = f'/content/training_progress_fold_SECOND{fold}.jpg'
    plt.savefig(training_progress_save_path)
    plt.close()

    history.append(h)

    # Predictions and metrics
    y_pred = conv_mixer_model.predict(dataset[fold]['validation'].batch(hyperparameters['batch_size']))
    y_pred_max = np.argmax(y_pred, axis=1)
    y_true = np.concatenate([label.numpy() for _, label in dataset[fold]['validation'].batch(hyperparameters['batch_size'])])

    # Generate and save confusion matrix
    cm_matrix = metrics.confusion_matrix(y_true, y_pred_max)
    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm_matrix, display_labels=['MCI', 'CN'])
    cm_display.plot()
    plt.title(f'Fold {fold}')
    cmatrix_save_path = f'/content/cmatrix_fold_SECOND{fold}.jpg'
    plt.savefig(cmatrix_save_path)
    plt.close()

    # Save classification report as CSV
    evaluation_dict = metrics.classification_report(y_true, y_pred_max, target_names=['MCI', 'CN'], output_dict=True)
    df2 = pd.DataFrame.from_dict(evaluation_dict)
    cmatrix_report_save_path = f'/content/cmatrix_report_fold{fold}.csv'
    df2.to_csv(cmatrix_report_save_path, index=False)

    # Clear model and session to release memory
    del conv_mixer_model
    K.clear_session()
    gc.collect()



